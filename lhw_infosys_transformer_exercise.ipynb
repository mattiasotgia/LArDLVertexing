{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "# Set some random seeds so that we should get the same answer! This method\n",
        "# sets the keras, python and numpy random seeds\n",
        "keras.utils.set_random_seed(42)\n",
        "\n",
        "# Get the ANKI English to Spanish dataset\n",
        "text_file = keras.utils.get_file(\n",
        "    fname=\"spa-eng.zip\",\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "import pathlib\n",
        "data_path = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NBJgdkhbCkw",
        "outputId": "dc5f1a3e-de3c-48ab-fb51-c2b51e002a85"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n",
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2638744/2638744 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(data_path) as data_file:\n",
        "    lines = data_file.read().split(\"\\n\")[:-1]\n",
        "\n",
        "import re\n",
        "# Now convert the lines into English to Spanish pairs\n",
        "# Since this is a simple model, it makes sense to enforce lower case and\n",
        "# remove any punctuation\n",
        "english_spanish_pairs = []\n",
        "max_length_english = 0\n",
        "for line in lines:\n",
        "    line = line.lower()\n",
        "    # Below regex removes characters not in the list, don't forget tabs (\\t)!\n",
        "    # We need to make sure that we keep the special characters from Spanish\n",
        "    line = re.sub(r'[^A-Za-z0-9 \\t√°√©√≠√≥√∫√±]+', '', line)\n",
        "    english, spanish = line.split(\"\\t\")\n",
        "    # As seen in the lecture, we need to add start and end tokens for Spanish\n",
        "    spanish = \"<sos> \" + spanish + \" <eos>\"\n",
        "    english_spanish_pairs.append((english, spanish))\n"
      ],
      "metadata": {
        "id": "d3bP4ih_bIe6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the dataset and split into training and validation samples (80%/20%)\n",
        "random.shuffle(english_spanish_pairs)\n",
        "# Let's have a look at a couple of examples\n",
        "print('We have', len(english_spanish_pairs), 'examples in the dataset')\n",
        "for w in range (5):\n",
        "    print('Example',w,':', english_spanish_pairs[w])\n",
        "\n",
        "num_training = int(0.8*len(english_spanish_pairs))\n",
        "training_pairs = english_spanish_pairs[:num_training]\n",
        "validation_pairs = english_spanish_pairs[num_training:]\n",
        "\n",
        "print(\"Training sample size =\", len(training_pairs),\n",
        "      \"and validation sample =\", len(validation_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrNcq_0Md8qR",
        "outputId": "aa40d4c0-38b1-463b-cf14-38ac5e4a0ce0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 118964 examples in the dataset\n",
            "Example 0 : ('its not in my contract', '<sos> no est√° en mi contrato <eos>')\n",
            "Example 1 : ('tom has the right to vote', '<sos> tom tiene el derecho a votar <eos>')\n",
            "Example 2 : ('my car is covered with pigeon poop', '<sos> mi coche est√° cubierto de caca de paloma <eos>')\n",
            "Example 3 : ('tom went to paris to study french', '<sos> tom fue a par√≠s para estudiar franc√©s <eos>')\n",
            "Example 4 : ('you dont have to do anything you dont want to', '<sos> no tienes que hacer nada que no quieras <eos>')\n",
            "Training sample size = 95171 and validation sample = 23793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the maximum vocabulary size that we want to use. If there\n",
        "# are more words than this, then the 15000 most frequent words are\n",
        "# used as the vocabulary\n",
        "vocab_size_english = 15000\n",
        "vocab_size_spanish = 15000\n",
        "\n",
        "# Define what the maximum sentence length is. If the sentence has\n",
        "# fewer words then it is padded wih zeros during the vectorisation\n",
        "sequence_length = 20\n",
        "\n",
        "# This layer simply converts a word into an integer\n",
        "english_vectorisation = keras.layers.TextVectorization(\n",
        "                        max_tokens=vocab_size_english,\n",
        "                        output_mode=\"int\",\n",
        "                        output_sequence_length=sequence_length,\n",
        "                        standardize=None)\n",
        "# The start / end tokens expand our sequences by one. It isn't two because\n",
        "# we don't use the <sos> token in the decoder output, or the <eos> token\n",
        "# in the decoder input\n",
        "spanish_vectorisation = keras.layers.TextVectorization(\n",
        "                        max_tokens=vocab_size_spanish,\n",
        "                        output_mode='int',\n",
        "                        output_sequence_length=sequence_length+1,\n",
        "                        standardize=None)\n",
        "\n",
        "# We need to separate out our language pairs to build the vocabulary\n",
        "english_words = []\n",
        "spanish_words = []\n",
        "for pair in english_spanish_pairs:\n",
        "  english_words.append(pair[0])\n",
        "  spanish_words.append(pair[1])\n",
        "\n",
        "# Actually build the vocabulary\n",
        "english_vectorisation.adapt(english_words)\n",
        "spanish_vectorisation.adapt(spanish_words)\n"
      ],
      "metadata": {
        "id": "1JBUGCGigNKQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the batch size now since the dataset needs to know\n",
        "batch_size = 64\n",
        "\n",
        "# We need to format our datasets a little more. We actually need three types\n",
        "# of sentences\n",
        "# 1) \"encoder_inputs\": english sentences\n",
        "# 2) \"decoder_inputs\": spanish sentences with <sos> for teacher forcing\n",
        "# 3) target sentence to predict: spanish sentences with <eos> at the end\n",
        "# A simple way to use this data and allow batching is a tf.Data.Dataset object\n",
        "def format_dataset(english, spanish_input, spanish_target):\n",
        "    inputs = {\"encoder_inputs\": english_vectorisation(english),\n",
        "              \"decoder_inputs\": spanish_vectorisation(spanish_input),}\n",
        "    target = spanish_vectorisation(spanish_target)\n",
        "    return (inputs, target,)\n",
        "\n",
        "def make_dataset(pairs, batch_size):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    # Decoder input needs to remove the last word (<eos>)\n",
        "    spanish_input = list(spa_texts)\n",
        "    for word in range(len(spanish_input)):\n",
        "        spanish_input[word] = spanish_input[word].rsplit(' ', 1)[0]\n",
        "    # Target needs to remove the first word (<sos>)\n",
        "    spanish_target = list(spa_texts)\n",
        "    for word in range(len(spanish_target)):\n",
        "        spanish_target[word] = spanish_target[word].split(' ', 1)[1]\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(eng_texts), spanish_input, spanish_target))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.cache().shuffle(2048).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "training_dataset = make_dataset(training_pairs, batch_size)\n",
        "validation_dataset = make_dataset(validation_pairs, batch_size)"
      ],
      "metadata": {
        "id": "yvZX8iVEm0hH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in training_dataset.take(1):\n",
        "    print('inputs[\"encoder_inputs\"] shape: ', inputs[\"encoder_inputs\"].shape)\n",
        "    print('inputs[\"decoder_inputs\"] shape: ', inputs[\"decoder_inputs\"].shape)\n",
        "    print('targets shape: ', targets.shape)\n",
        "    print(inputs[\"encoder_inputs\"][0])\n",
        "    print(inputs[\"decoder_inputs\"][0])\n",
        "    print(targets[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fuNT8OWttwI",
        "outputId": "0b931022-91a0-4eca-e47e-391db28bb2f1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs[\"encoder_inputs\"] shape:  (64, 20)\n",
            "inputs[\"decoder_inputs\"] shape:  (64, 21)\n",
            "targets shape:  (64, 21)\n",
            "tf.Tensor(\n",
            "[   3 1194   49 2478  195  110   52    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0], shape=(20,), dtype=int64)\n",
            "tf.Tensor(\n",
            "[   2 5393   18 1345    5  608   32  233  856    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0], shape=(21,), dtype=int64)\n",
            "tf.Tensor(\n",
            "[5393   18 1345    5  608   32  233  856    3    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0], shape=(21,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "We have (finally) formatted the training and validation data into a format we can use. Now let's get on with defining all of the components that we need for our transformer.\n",
        "1. Word embedding and position encoding\n",
        "2. Multi-head attention\n",
        "3. Feed-forward network\n",
        "4. The encoder\n",
        "5. The decoder\n",
        "6. The transformer (brings together all of the above)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "vmkxOSsXsz6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Firstly, let's make sinusiodal position encoding layer. We will actually\n",
        "# used a learned position encoding, but I wanted to include this function\n",
        "# for completeness as it was used in the original transformer\n",
        "class SinusoidalPositionEncoding(keras.layers.Layer):\n",
        "    def __init__(self, sequence_length, d_model, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.l = sequence_length\n",
        "        self.d_model = d_model\n",
        "\n",
        "    # Inputs doesn't do anything, but needed to make other interfaces\n",
        "    def call(self, inputs):\n",
        "        position = np.arange(self.l)[:, np.newaxis]\n",
        "        # Create div_term array with shape (d_model//2,)\n",
        "        # which corresponds to the denominators raised to the power 2i/d_model\n",
        "        div_term = np.exp(-np.log(10000.0) * (np.arange(0, self.d_model, 2) / self.d_model))\n",
        "        # Initialize the position encoding array\n",
        "        encoded_position = np.zeros((self.l, self.d_model))\n",
        "        # Compute sinusoidal values using broadcasting\n",
        "        encoded_position[:, 0::2] = np.sin(position * div_term)\n",
        "        encoded_position[:, 1::2] = np.cos(position * div_term)\n",
        "        return encoded_position"
      ],
      "metadata": {
        "id": "eMF0R-JefIZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding layer that does the embedding and position encoding. We can choose\n",
        "# whether to use the learned or sinusoidal encoding.\n",
        "class EmbedAndEncode(keras.layers.Layer):\n",
        "    def __init__(self, sequence_length, d_model, vocab_size, learned_encoding, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Embedding layers\n",
        "        self.l = sequence_length\n",
        "        self.embedding = keras.layers.Embedding(vocab_size, d_model)\n",
        "        if learned_encoding:\n",
        "            self.position = keras.layers.Embedding(sequence_length, d_model)\n",
        "        else:\n",
        "            self.position = SinusoidalPositionEncoding(sequence_length, d_model)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        e = self.embedding(inputs)\n",
        "        positions = np.arange(0, self.l, 1)\n",
        "        p = self.position(positions)\n",
        "        x = p + e\n",
        "        return x"
      ],
      "metadata": {
        "id": "KBvDI9roeQQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "In this next block we will build the attention mechanism. This is the key component of the transformer. To recap from the lectures, there are a few steps that we need to follow.\n",
        "\n",
        "\n",
        "1.   We get three inputs passed into the attention heads, which then pass through the $W^Q$, $W^K$ and $W^V$ matrices, which all have dimensions $\\left( d_\\textrm{model}, d_k \\right)$ to give us queries $Q$, keys $K$, and values $V$.\n",
        "2.   Calculate $A = \\textrm{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right)$\n",
        "3.   The output from the attention head if given by the matrix product $AV$\n",
        "4.   Repeat the above for each attention head and concatenate the output\n",
        "5.   Pass the concatenated output through the dense layer representing matrix $W^0$ with shape $\\left( \\left(n_\\textrm{heads}d_k \\right) \\times d_\\textrm{model}\\right)$\n",
        "\n",
        "Implementation information:\n",
        "\n",
        "*   We saw in the lectures how to represent the weight matrices as `Dense` layers, where the number of neurons is equal to the number of columns in the matrix. We need to ensure that we don't use a bias term and that we are using a linear (effectively the identity) activation function. This is the default in keras so we don't need to specify it in the code below.\n",
        "* Use `tf.matmul(A,B)` to multiply matrix $A$ by matrix $B$. It can take optional boolean arguments to transpose the matrices before multiplying (`transpose_a` and `transpose_b`)\n"
      ],
      "metadata": {
        "id": "cUoO5ZB82tgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ordinarily we could just use keras.layers.Attention or MultiHeadAttention\n",
        "# to build a transformer. However, in this case let's write our own layer\n",
        "# as an exercise.\n",
        "class CustomAttention(keras.layers.Layer):\n",
        "    def __init__(self, d_model, d_k, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Important values\n",
        "        self.d_k = d_k\n",
        "        self.num_heads = num_heads\n",
        "        # Q, K and V weight matrices for each head.\n",
        "        # Fill in the number of nodes in the W^Q, W^K and W^V dense layers\n",
        "        # by replacing the \"None\" values in the three lines below\n",
        "        self.Wq = [keras.layers.Dense(None, use_bias=False) for _ in range(self.num_heads)]\n",
        "        self.Wk = [keras.layers.Dense(None, use_bias=False) for _ in range(self.num_heads)]\n",
        "        self.Wv = [keras.layers.Dense(None, use_bias=False) for _ in range(self.num_heads)]\n",
        "        # Final weight matrix applied to concatenated output of all heads\n",
        "        self.W0 = keras.layers.Dense(None, use_bias=False)\n",
        "        # Softmax\n",
        "        self.softmax = keras.layers.Softmax()\n",
        "        # Layer normalisation\n",
        "        self.layernorm = keras.layers.LayerNormalization()\n",
        "        # Dropout\n",
        "        self.dropout = keras.layers.Dropout(0.1)\n",
        "        # Addition layer\n",
        "        self.add = keras.layers.Add()\n",
        "\n",
        "    # We take three inputs here, the values that will be projected into q, k and v\n",
        "    # For self-attention these are all the same, but for cross-attention q comes\n",
        "    # the masked attention, and k and v from the encoder.\n",
        "    def call(self, q, k, v, mask=None):\n",
        "        # Z will store the output from each head in a list\n",
        "        Z = []\n",
        "        # Loop over all heads\n",
        "        for head in range(self.num_heads):\n",
        "            # Q, K and V projections. Pass the correct inputs through the\n",
        "            # corresponding dense layers by replacing \"None\"\n",
        "            Q = self.Wq[head](None)\n",
        "            K = self.Wk[head](None)\n",
        "            V = self.Wv[head](None)\n",
        "            # Multiply Q by the transpose of K\n",
        "            QKT = None\n",
        "            # The mask is added to QKT at this point. This None is actually\n",
        "            # part of the code, and not something to change!\n",
        "            if mask is not None: # DON'T CHANGE THIS NONE!\n",
        "                QKT = self.add([QKT, mask])\n",
        "            # Now we apply the normalisation\n",
        "            QKT = QKT / tf.math.sqrt(float(self.d_k))\n",
        "            # Apply the softmax activation to get the attention matrix\n",
        "            A = None\n",
        "            A = self.dropout(A)\n",
        "            # Final output for each head. We need to multiply matrices A and V\n",
        "            output = None\n",
        "            # Add the result for this head to the list of results\n",
        "            Z.append(output)\n",
        "\n",
        "        # Concatenate outputs from the heads\n",
        "        concZ = tf.concat(Z, axis=-1)\n",
        "        # Mutliply by the final weight matrix W0 to give us a shape equal to the input\n",
        "        output = None\n",
        "        # Perform some dropout\n",
        "        output = self.dropout(output)\n",
        "        # Make the residual connection that connects input q to the output and\n",
        "        # normalise the weights\n",
        "        output = self.layernorm(self.add([q,output]))\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "7jRUM3OZj533"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Since this is the most important part of the transformer, let's perform a quick test to see if we get the expected answer. If your attention layer is correct then you will see the following output:\n",
        "\n",
        "A tensor of shape `(1, 5, 4)` with the following values:\n",
        "```\n",
        "[[[ 0.22355375  0.6402806  -1.6862876   0.822453  ]\n",
        "  [ 0.7611287   1.0543339  -1.4768872  -0.33857557]\n",
        "  [ 0.8607119   0.61222994 -1.68159     0.20864806]\n",
        "  [ 0.34176865  1.0672501  -1.634869    0.2258502 ]\n",
        "  [ 1.1459001   0.6712856  -1.4445571  -0.37262878]]]\n",
        "```\n",
        "As expected, the output from the layer has the same shape as the input.\n"
      ],
      "metadata": {
        "id": "EsyJZXxYYZFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is like a unit test of the layer\n",
        "fake_input = np.random.rand(1,5,4)\n",
        "test_model_input = keras.layers.Input(shape=(5,4), dtype=\"float32\")\n",
        "test_model_output = CustomAttention(d_model=4, d_k=3, num_heads=8)(test_model_input, test_model_input, test_model_input, mask=None)\n",
        "test_model = keras.Model(test_model_input, test_model_output)\n",
        "pred = test_model.predict((fake_input))\n",
        "print(pred.shape)\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "m4aP_v4c7wQ6",
        "outputId": "4f7eef25-7c30-41ea-990f-2d73da239cc1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-870090375b1d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This is like a unit test of the layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfake_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_model_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_model_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_model_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_model_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_model_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_model_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_model_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Now we create the feed forward network. This is a very simple network consisting of just two `Dense` layers. The first layer expands the dimensions of the input to `ff_dim` and the second contracts it back down to the size of the input, `d_model`."
      ],
      "metadata": {
        "id": "Oljz0d7_gh0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feed forward network including residual connection. This is just a two layer\n",
        "# neural network using dense layers\n",
        "class FeedForward(keras.layers.Layer):\n",
        "    def __init__(self, d_model, ff_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Dense layers - fill in the number of neurons\n",
        "        self.dense_1 = keras.layers.Dense(None, activation='relu')\n",
        "        self.dense_2 = keras.layers.Dense(None)\n",
        "        # Layer normalisation\n",
        "        self.layernorm = keras.layers.LayerNormalization()\n",
        "        # Dropout\n",
        "        self.dropout = keras.layers.Dropout(0.1)\n",
        "        # Addition layer\n",
        "        self.add = keras.layers.Add()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # First dense layer followed by some dropout\n",
        "        output = self.dense_1(inputs)\n",
        "        output = self.dropout(output)\n",
        "        # Second dense layer followed by some dropout\n",
        "        output = self.dense_2(output)\n",
        "        output = self.dropout(output)\n",
        "        # Perform the residual connection and normalise\n",
        "        output = self.layernorm(self.add([inputs,output]))\n",
        "        return output"
      ],
      "metadata": {
        "id": "DR-B-9Cgeyef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Let's add a second little test. We'll pass the output of the attention through the feed forward layer and see what we get.\n",
        "A tensor with shape `(1, 5, 4)` and values:\n",
        "```\n",
        "[[[ 0.01011109  0.74887556 -1.634079    0.87509245]\n",
        "  [ 0.48468572  1.1789591  -1.5345951  -0.12904972]\n",
        "  [ 0.592366    0.7920997  -1.7069805   0.32251465]\n",
        "  [ 0.10495934  1.130093   -1.6039987   0.36894622]\n",
        "  [ 0.9480371   0.85631114 -1.5087731  -0.29557496]]]\n",
        "  ```\n",
        "   Again, the output size will match the original input size, and the elements should be as above\n"
      ],
      "metadata": {
        "id": "yuBInHU99gid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model_output = FeedForward(d_model=4, ff_dim=20)(test_model_output)\n",
        "test_model = keras.Model(test_model_input, test_model_output)\n",
        "pred = test_model.predict((fake_input))\n",
        "print(pred.shape)\n",
        "print(pred)"
      ],
      "metadata": {
        "id": "9g-xOpmn9hsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Now we have our building blocks, we just need to put them together to make the encoder and decoder. The encoder is now very simple - we just need to perform two steps:\n",
        "1. Self-attention: The same input is used for the  `ùëû` ,  `ùëò`  and  `ùë£`  inputs to the `CustomAttention(q, k, v, mask)` layer we defined earlier. We need to make sure to pass on the `mask` here too.\n",
        "2. The output from the self-attention goes through the `FeedForward(d_model, ff_dim)` layer."
      ],
      "metadata": {
        "id": "avWTp6g6FYyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we have the building blocks that we need, let's make our Encoder\n",
        "class Encoder(keras.layers.Layer):\n",
        "    def __init__(self, d_model, d_k, ff_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Attention\n",
        "        self.attention = None\n",
        "        # Feed-forward network\n",
        "        self.feedforward = None\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        # Call the attention function with the correct arguments\n",
        "        z = None\n",
        "        # Feed forward network\n",
        "        output = None\n",
        "        return output"
      ],
      "metadata": {
        "id": "DzRdvtCkj_ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "The decoder is marginally more complex than the encoder, since we need to have two attention layers:\n",
        "1. Masked self-attention: input to the decoder is used for the `ùëû`, `ùëò` and `ùë£` inputs to the `CustomAttention` layer we defined earlier. We need to make sure we are using the mask for the decoder input here.\n",
        "2. Cross-attention: this uses the output of the encoder as the $k$ and $v$ inputs, but the output of the masked attention as $q$. We use the `CustomAttention(q, k, v, mask)` layer again here, making sure to use the mask that we created for the encoder input.\n",
        "3. The output is passed into a feed-forward network to give the final ouput of the decoder, using the `FeedForward(d_model, ff_dim)` layer.\n"
      ],
      "metadata": {
        "id": "AL9-Tp9qF8zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's make the decoder.\n",
        "class Decoder(keras.layers.Layer):\n",
        "    def __init__(self, d_model, d_k, ff_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Attention, both masked and cross\n",
        "        self.masked_attention = None\n",
        "        self.cross_attention = None\n",
        "        # Feed-forward network\n",
        "        self.feedforward = None\n",
        "\n",
        "    # We have the encoder output to include here\n",
        "    def call(self, inputs, encoder_output, decoder_mask=None, encoder_mask=None):\n",
        "        # Call the masked attention function with the correct input and mask\n",
        "        z = None\n",
        "        # Call the cross-attention function with the correct input and mask\n",
        "        c = None\n",
        "        # Feed forward network\n",
        "        output = None\n",
        "        return output"
      ],
      "metadata": {
        "id": "tHL1Sd9ethSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Now all we need to do is put the building blocks together to create the full transformer model.\n",
        "1. Create the three masks that we need to make use of:\n",
        "> 1.   Padding mask for the encoder input\n",
        "> 2.   Padding mask for the decoder input\n",
        "> 3.   Causal mask for the decoder (prevents us looking into the future when predicting the output)\n",
        "2. Perform the sentence embedding for encoder (English) and decoder (Spanish) inputs\n",
        "3. Call the encoder with the English input and mask\n",
        "4. Call the decoder with the Spanish input, theoutput from the encoder, the combination of the decoder masks and the encoder mask\n",
        "5. Make the predictions from the output of the decoder\n",
        "6. Sit back and enjoy some translations soon"
      ],
      "metadata": {
        "id": "kI4uoO6xD1Jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally we build the transformer\n",
        "class Transformer(keras.layers.Layer):\n",
        "    def __init__(self, sequence_length, d_model, d_k, ff_dim, num_heads, vocab_size_english, vocab_size_spanish, learned_encoding, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # The sequence lengths for the encoder and decoder inputs\n",
        "        self.l_e = sequence_length\n",
        "        self.l_d = sequence_length + 1\n",
        "        # Layers to encode the inputs for the encoder and decoder\n",
        "        self.english_embed = EmbedAndEncode(self.l_e, d_model, vocab_size_english, learned_encoding)\n",
        "        self.spanish_embed = EmbedAndEncode(self.l_d, d_model, vocab_size_spanish, learned_encoding)\n",
        "        # The encoder and decoder\n",
        "        self.encoder = Encoder(d_model, d_k, ff_dim, num_heads)\n",
        "        self.decoder = Decoder(d_model, d_k, ff_dim, num_heads)\n",
        "        self.dropout = keras.layers.Dropout(0.1)\n",
        "        # The final output layer to make the preditions\n",
        "        self.classifier = keras.layers.Dense(vocab_size_spanish, activation='softmax')\n",
        "\n",
        "    def call(self, enc_in, dec_in):\n",
        "        # We need to create masks before making the embeddings to prevent the\n",
        "        # attention mechanism from considering padded values\n",
        "        encoder_padding_mask = self.create_padding_mask(enc_in)\n",
        "        decoder_padding_mask = self.create_padding_mask(dec_in)\n",
        "        decoder_causal_mask = self.create_causal_mask()\n",
        "        # Combine the two masks for the decoder\n",
        "        decoder_mask = tf.minimum(decoder_padding_mask, decoder_causal_mask)\n",
        "        # Prepare the encoder input and run it\n",
        "        enc_in = self.english_embed(enc_in)\n",
        "        enc_in = self.dropout(enc_in)\n",
        "        enc_out = self.encoder(enc_in, encoder_padding_mask)\n",
        "        # Prepare the decoder input and run it\n",
        "        dec_in = self.spanish_embed(dec_in)\n",
        "        dec_in = self.dropout(dec_in)\n",
        "        dec_out = self.decoder(dec_in, enc_out, decoder_mask, encoder_padding_mask)\n",
        "        # Make the predictions\n",
        "        dec_out = self.dropout(dec_out)\n",
        "        dec_out = self.classifier(dec_out)\n",
        "        return dec_out\n",
        "\n",
        "    def create_causal_mask(self):\n",
        "        # The causal mask is an upper triangular matrix with values set\n",
        "        # to a very large negative number\n",
        "        mask = np.triu(np.ones((self.l_d, self.l_d)) * -1.0e20, k=1)\n",
        "        return mask\n",
        "\n",
        "    def create_padding_mask(self, inputs):\n",
        "        # Determine which positions are non-zero\n",
        "        mask = tf.math.equal(inputs, 0)\n",
        "        mask = tf.cast(mask, tf.float64)\n",
        "        # Ensure the mask is flexible enough to broadcast over different size matrices\n",
        "        mask = mask[:, tf.newaxis, :]\n",
        "        # Set the values we want to mask to a very large negative number\n",
        "        mask *= -1.0e20\n",
        "        return mask"
      ],
      "metadata": {
        "id": "SGTu4__K-5J2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Now lets define the parameters of the model and build it. To keep things reasonably sized. At some point you could try changing these if you have a GPU that you can use for training, but you will need to use these values to load my model weights later.\n",
        "\n",
        "*   $d_\\textrm{model} = 256$\n",
        "*   $n_\\textrm{heads} = 8$\n",
        "*   $d_k = 32$\n",
        "*   $\\textrm{ff_dim} = 1024$\n",
        "\n",
        "In this example I have set $d_k = d_v$, so we won't see $d_v$ in this code. This follows the method used in the original transformer, but you could modify the layers above to allow for a different value of $d_v$\n",
        "\n",
        "Using these values, you should find that the network has `13385624` parameters\n"
      ],
      "metadata": {
        "id": "GkJVpCIsFfWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill in the model parameter values. Use those given about if you want to be\n",
        "# able to load that weights from my trained network later\n",
        "d_model = None\n",
        "num_heads = None\n",
        "d_k = None\n",
        "ff_dim = None\n",
        "\n",
        "# Define the two input layers that we need for the model\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "\n",
        "transformer_outputs = Transformer(sequence_length, d_model, d_k, ff_dim, num_heads, vocab_size_english, vocab_size_spanish, learned_encoding=True)(encoder_inputs,decoder_inputs)\n",
        "transformer = keras.Model([encoder_inputs,decoder_inputs], transformer_outputs)\n",
        "transformer.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm9TiiHP2iCy",
        "outputId": "3fa99ef4-4be4-497a-90b6-6c78a4c59fdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " transformer (Transformer)      (None, 21, 15000)    13385624    ['encoder_inputs[0][0]',         \n",
            "                                                                  'decoder_inputs[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 13,385,624\n",
            "Trainable params: 13,385,624\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can use a learning rate scheduler to reduce the learning rate if necessary\n",
        "# Not really necessary here as it will take quite a few epochs to be useful,\n",
        "# but I leave it here for completeness\n",
        "lr_schedule = keras.callbacks.ReduceLROnPlateau(patience=3)\n",
        "\n",
        "# Use the optimiser settings from the original paper (but not the lr mechanism)\n",
        "optimiser = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.98, epsilon=1.0e-9)\n",
        "\n",
        "# This custom loss function helps us to converge better. We ignore the padding class\n",
        "# in the loss function and smooth the labels a little. It would still work ok\n",
        "# just using the standard keras.losses.SparseCategoricalCrossentropy() loss\n",
        "def masked_and_padded_scce_loss(y_true, y_pred, label_smoothing=0.1):\n",
        "    # Convert from a sparse to a one-hot representation of the truth\n",
        "    y_true_one_hot = tf.one_hot(tf.cast(y_true, tf.int32), depth=tf.shape(y_pred)[-1])\n",
        "    # We want to ignore the padding class, which is the first element of the one-hot vector\n",
        "    smoothed_loss = tf.keras.losses.categorical_crossentropy(\n",
        "        y_true_one_hot[:,:,1:], y_pred[:,:,1:], from_logits=False,\n",
        "        label_smoothing=label_smoothing)\n",
        "    return smoothed_loss\n",
        "\n",
        "# Compile the model ready for training\n",
        "transformer.compile(optimiser, loss=masked_and_padded_scce_loss, metrics=[\"accuracy\"])\n"
      ],
      "metadata": {
        "id": "E8oYLo6epOkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Now we can train the network! Unfortunately this will be rather slow so in the next block you can load some weights from a network I trained previously. One a V100 it trains very quickly - less than 1 minute per epoch! If you have a GPU available, go ahead and train it for 10 epochs or so, otherwise feel free to skip this block\n"
      ],
      "metadata": {
        "id": "WyIYlJMUeDxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can actually train our transformer!\n",
        "num_epochs = 1\n",
        "transformer.fit(training_dataset, epochs=num_epochs, validation_data=validation_dataset, callbacks=[lr_schedule])\n",
        "# Save the weights if you want to keep them\n",
        "transformer.save_weights(\"my_transformer_weights.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "7My5RhbFO9Xi",
        "outputId": "3c882421-c902-43a2-f1b9-5f9811d728c2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3c3ae3f7896f>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# so in the next block you can load some weights from a network I trained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlr_schedule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mweights_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"eng_spa_transformer_weights_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mff_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".keras\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'transformer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Training is unfortunately very slow on a CPU, so if you don't have any GPU access then feel free to load some model weights that I produced earlier\n"
      ],
      "metadata": {
        "id": "1ytwdb1rdz2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instead of training, we can load my weights\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1MPdEgvcCSCvKrrzWON_L0ILVaRwa2G0N' -O lhw_weights.keras\n",
        "transformer.load_weights(\"lhw_weights.keras\")"
      ],
      "metadata": {
        "id": "6jlNSoJyqWyO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "outputId": "def2021e-28ef-49a2-941f-21d8e35bfb46"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-05 08:41:02--  https://docs.google.com/uc?export=download&id=1MPdEgvcCSCvKrrzWON_L0ILVaRwa2G0N\n",
            "Resolving docs.google.com (docs.google.com)... 173.194.74.113, 173.194.74.138, 173.194.74.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|173.194.74.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-00-0o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/lhfijj16d4bpu9f6lds61rnlmedld1ou/1693903200000/15250864113763849298/*/1MPdEgvcCSCvKrrzWON_L0ILVaRwa2G0N?e=download&uuid=291d970f-aad9-4782-b176-0e41922a4e99 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-09-05 08:41:05--  https://doc-00-0o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/lhfijj16d4bpu9f6lds61rnlmedld1ou/1693903200000/15250864113763849298/*/1MPdEgvcCSCvKrrzWON_L0ILVaRwa2G0N?e=download&uuid=291d970f-aad9-4782-b176-0e41922a4e99\n",
            "Resolving doc-00-0o-docs.googleusercontent.com (doc-00-0o-docs.googleusercontent.com)... 142.251.161.132, 2607:f8b0:4001:c5c::84\n",
            "Connecting to doc-00-0o-docs.googleusercontent.com (doc-00-0o-docs.googleusercontent.com)|142.251.161.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53693976 (51M) [application/octet-stream]\n",
            "Saving to: ‚Äòlhw_weights.keras‚Äô\n",
            "\n",
            "lhw_weights.keras   100%[===================>]  51.21M   137MB/s    in 0.4s    \n",
            "\n",
            "2023-09-05 08:41:05 (137 MB/s) - ‚Äòlhw_weights.keras‚Äô saved [53693976/53693976]\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-52af683585cb>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Instead of training, we can load my weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1MPdEgvcCSCvKrrzWON_L0ILVaRwa2G0N' -O lhw_weights.keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lhw_weights.keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'transformer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Now all we need to do is look at how to perform inference. As I said in the lectures, this is done in an iterative way. We pass the english sentence into the encoder, and then pass the `<sos>` (start of sentence) token to the decoder. The decoder then predicts the output word, which we then append to the `<sos>` token and run things again with this new input."
      ],
      "metadata": {
        "id": "LeanDcqZ6uiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get our vocabulary from the vectorisation layer we made right back at the top\n",
        "# of the notebook\n",
        "spanish_vocab = spanish_vectorisation.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spanish_vocab)), spanish_vocab))\n",
        "\n",
        "def run_inference(input_sentence):\n",
        "    # Get the tokenised input sentence ready for embedding\n",
        "    encoded_input = english_vectorisation([input_sentence])\n",
        "    # The input to the decoder is initially just \"<sos>\". In the second\n",
        "    # iteration the input becomes \"<sos> <first_predicted_word>\"\n",
        "    decoded_sentence = \"<sos>\"\n",
        "    for i in range(sequence_length):\n",
        "        decoder_input = spanish_vectorisation([decoded_sentence])\n",
        "        predictions = transformer([encoded_input, decoder_input])\n",
        "        # Get the index of the word with the highest probability. The\n",
        "        # index i here ensures we look at the correct row of the output\n",
        "        # since it has size (1, len(decoded_sentence), vocab_size_spanish)\n",
        "        predicted_word_index = np.argmax(predictions[0, i, :])\n",
        "        predicted_word = spa_index_lookup[predicted_word_index]\n",
        "        decoded_sentence += \" \" + predicted_word\n",
        "\n",
        "        if predicted_word == \"<eos>\":\n",
        "            break\n",
        "    # Print the input and output, removing the <sos> and <eos> tokens\n",
        "    print(input_sentence,\" :: \",decoded_sentence[6:-6])"
      ],
      "metadata": {
        "id": "QZtnPxHPWE8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for the fun part! I've added a few sentences for the transformer to try to translate. The first sentence is, of course, the one from the lectures. Feel free to play around and add your own sentences and see how it does (if you can't speak spanish, you could ask Lorena, Zahari, Google, and in a crisis, me.)\n",
        "\n",
        "If you've loaded my weights, then the five sentences below are translated correctly. As a bit of a language technicality, in the lectures I said that `I have a big cat` should translate to `Yo tengo un gato grande`. You may notice that `Yo`, meaning `I`, is missing - pronouns such as `yo` are often dropped from spanish since the conjugation of the verb (`tengo`) tells you that the pronoun is `yo` anyway."
      ],
      "metadata": {
        "id": "T-51tDxng0Hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_inference(\"i have a big cat\")\n",
        "run_inference(\"i have a small dog\")\n",
        "run_inference(\"are there horses here\")\n",
        "run_inference(\"my car is broken\")\n",
        "run_inference(\"im going to translate this sentence\")\n",
        "run_inference(\"Please translate this sentence\")"
      ],
      "metadata": {
        "id": "t7UABLDR6ASt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "efca3d16-bdaa-4706-b399-bd5ddea57e51"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-f39f12aa8c6a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i have a big cat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrun_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i have a small dog\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrun_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"are there horses here\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrun_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"my car is broken\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrun_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"im going to translate this sentence\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'run_inference' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xKNX_wgssSbe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}